{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MHdn5TY7chkH",
        "outputId": "9aff8fa1-5743-4c18-b0d1-ef9e0901a2f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoQ1K2i3arem"
      },
      "source": [
        "# Testing Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_I2-dK23areo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0CLGb5pTarep"
      },
      "outputs": [],
      "source": [
        "def perform_corrected_internal_testing():\n",
        "    \"\"\"Perform internal testing using the original training datasets with train/test split\"\"\"\n",
        "    print(\"üî¨ STARTING CORRECTED INTERNAL TESTING ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load all necessary components\n",
        "    base_path = r'/content/drive/MyDrive/Research/TB_new/Saved_files'\n",
        "\n",
        "    components = {}\n",
        "    try:\n",
        "        # Load models\n",
        "        with open(os.path.join(base_path, 'best_sub_model_1.pkl'), 'rb') as file:\n",
        "            components['model_TB_Status'] = pickle.load(file)\n",
        "        with open(os.path.join(base_path, 'best_sub_model_2.pkl'), 'rb') as file:\n",
        "            components['model_TB_Type'] = pickle.load(file)\n",
        "        with open(os.path.join(base_path, 'best_sub_model_3.pkl'), 'rb') as file:\n",
        "            components['model_TB_Stage'] = pickle.load(file)\n",
        "\n",
        "        # Load feature selectors\n",
        "        with open(os.path.join(base_path, 'feature_selector_TB_Status.pkl'), 'rb') as file:\n",
        "            components['feature_selector_TB_Status'] = pickle.load(file)\n",
        "        with open(os.path.join(base_path, 'feature_selector_TB_Type.pkl'), 'rb') as file:\n",
        "            components['feature_selector_TB_Type'] = pickle.load(file)\n",
        "        with open(os.path.join(base_path, 'feature_selector_TB_Stage.pkl'), 'rb') as file:\n",
        "            components['feature_selector_TB_Stage'] = pickle.load(file)\n",
        "\n",
        "        # Load label encoders\n",
        "        with open(os.path.join(base_path, 'label_encoder_TB_Status.pkl'), 'rb') as file:\n",
        "            components['label_encoder_TB_Status'] = pickle.load(file)\n",
        "        with open(os.path.join(base_path, 'label_encoder_TB_Type.pkl'), 'rb') as file:\n",
        "            components['label_encoder_TB_Type'] = pickle.load(file)\n",
        "        with open(os.path.join(base_path, 'label_encoder_TB_Stage.pkl'), 'rb') as file:\n",
        "            components['label_encoder_TB_Stage'] = pickle.load(file)\n",
        "\n",
        "        # Load scalers\n",
        "        scaler_path = os.path.join(base_path, 'scalar')\n",
        "        with open(os.path.join(scaler_path, 'scaler_TB_Status.pkl'), 'rb') as file:\n",
        "            components['scaler_TB_Status'] = pickle.load(file)\n",
        "        with open(os.path.join(scaler_path, 'scaler_TB_Type.pkl'), 'rb') as file:\n",
        "            components['scaler_TB_Type'] = pickle.load(file)\n",
        "        with open(os.path.join(scaler_path, 'scaler_TB_Stage.pkl'), 'rb') as file:\n",
        "            components['scaler_TB_Stage'] = pickle.load(file)\n",
        "\n",
        "        print(\"‚úÖ All models and components loaded successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading components: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # Load selected features\n",
        "    selected_features = {}\n",
        "    try:\n",
        "        selected_features['TB_Status'] = pd.read_csv(\n",
        "            os.path.join(base_path, 'selected_features_TB_HC_OD.csv')\n",
        "        )['Selected Features'].tolist()\n",
        "        selected_features['TB_Type'] = pd.read_csv(\n",
        "            os.path.join(base_path, 'selected_features_PTB_EPTB.csv')\n",
        "        )['Selected Features'].tolist()\n",
        "        selected_features['TB_Stage'] = pd.read_csv(\n",
        "            os.path.join(base_path, 'selected_features_ATB_LTB.csv')\n",
        "        )['Selected Features'].tolist()\n",
        "        print(\"‚úÖ Selected features loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading selected features: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # Perform testing for each model using their respective datasets\n",
        "    results = {}\n",
        "    detailed_results = {}\n",
        "\n",
        "    # Model 1: TB Status Testing using TB_HC_OD dataset\n",
        "    print(\"\\nüìä TESTING TB STATUS MODEL (TB_HC_OD Dataset)...\")\n",
        "    try:\n",
        "        # Load the TB_HC_OD dataset\n",
        "        tb_hc_od_path = r'..\\..\\Datasets\\train\\balanced\\TB_HC_OD.csv'\n",
        "        tb_hc_od_data = pd.read_csv(tb_hc_od_path)\n",
        "        print(f\"   ‚úÖ TB_HC_OD dataset loaded: {tb_hc_od_data.shape}\")\n",
        "\n",
        "        # Prepare data\n",
        "        X = tb_hc_od_data[selected_features['TB_Status']]\n",
        "        y = tb_hc_od_data['TB_Status']\n",
        "\n",
        "        # Split data (same as during training)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Apply feature selection to test data\n",
        "        X_test_selected = components['feature_selector_TB_Status'].transform(X_test)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred_encoded = components['model_TB_Status'].predict(X_test_selected)\n",
        "        y_pred = components['label_encoder_TB_Status'].inverse_transform(y_pred_encoded)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        results['TB_Status'] = {\n",
        "            'accuracy': accuracy,\n",
        "            'initial_features': len(selected_features['TB_Status']),\n",
        "            'final_features': X_test_selected.shape[1],\n",
        "            'samples': len(y_test),\n",
        "            'dataset': 'TB_HC_OD'\n",
        "        }\n",
        "\n",
        "        detailed_results['TB_Status'] = {\n",
        "            'true_labels': y_test,\n",
        "            'pred_labels': y_pred,\n",
        "            'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
        "        }\n",
        "\n",
        "        print(f\"   ‚úÖ Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"   ‚úÖ Features: {len(selected_features['TB_Status'])} ‚Üí {X_test_selected.shape[1]}\")\n",
        "        print(f\"   ‚úÖ Test Samples: {len(y_test)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error in TB Status testing: {e}\")\n",
        "        results['TB_Status'] = {'error': str(e)}\n",
        "\n",
        "    # Model 2: TB Type Testing using PTB_EPTB dataset\n",
        "    print(\"\\nüìä TESTING TB TYPE MODEL (PTB_EPTB Dataset)...\")\n",
        "    try:\n",
        "        # Load the PTB_EPTB dataset\n",
        "        ptb_eptb_path = r'..\\..\\Datasets\\train\\balanced\\PTB_EPTB.csv'\n",
        "        ptb_eptb_data = pd.read_csv(ptb_eptb_path)\n",
        "        print(f\"   ‚úÖ PTB_EPTB dataset loaded: {ptb_eptb_data.shape}\")\n",
        "\n",
        "        # Prepare data\n",
        "        X = ptb_eptb_data[selected_features['TB_Type']]\n",
        "        y = ptb_eptb_data['TB_Status']  # This should be Pulmonary TB vs Extra Pulmonary TB\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Apply feature selection to test data\n",
        "        X_test_selected = components['feature_selector_TB_Type'].transform(X_test)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred_encoded = components['model_TB_Type'].predict(X_test_selected)\n",
        "        y_pred = components['label_encoder_TB_Type'].inverse_transform(y_pred_encoded)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        results['TB_Type'] = {\n",
        "            'accuracy': accuracy,\n",
        "            'initial_features': len(selected_features['TB_Type']),\n",
        "            'final_features': X_test_selected.shape[1],\n",
        "            'samples': len(y_test),\n",
        "            'dataset': 'PTB_EPTB'\n",
        "        }\n",
        "\n",
        "        detailed_results['TB_Type'] = {\n",
        "            'true_labels': y_test,\n",
        "            'pred_labels': y_pred,\n",
        "            'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
        "        }\n",
        "\n",
        "        print(f\"   ‚úÖ Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"   ‚úÖ Features: {len(selected_features['TB_Type'])} ‚Üí {X_test_selected.shape[1]}\")\n",
        "        print(f\"   ‚úÖ Test Samples: {len(y_test)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error in TB Type testing: {e}\")\n",
        "        results['TB_Type'] = {'error': str(e)}\n",
        "\n",
        "    # Model 3: TB Stage Testing using ATB_LTB dataset\n",
        "    print(\"\\nüìä TESTING TB STAGE MODEL (ATB_LTB Dataset)...\")\n",
        "    try:\n",
        "        # Load the ATB_LTB dataset\n",
        "        atb_ltb_path = r'..\\..\\Datasets\\train\\balanced\\ATB_LTB.csv'\n",
        "        atb_ltb_data = pd.read_csv(atb_ltb_path)\n",
        "        print(f\"   ‚úÖ ATB_LTB dataset loaded: {atb_ltb_data.shape}\")\n",
        "\n",
        "        # Prepare data\n",
        "        X = atb_ltb_data[selected_features['TB_Stage']]\n",
        "        y = atb_ltb_data['TB_Status']  # This should be Active TB vs Latent TB\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Apply feature selection to test data\n",
        "        X_test_selected = components['feature_selector_TB_Stage'].transform(X_test)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred_encoded = components['model_TB_Stage'].predict(X_test_selected)\n",
        "        y_pred = components['label_encoder_TB_Stage'].inverse_transform(y_pred_encoded)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        results['TB_Stage'] = {\n",
        "            'accuracy': accuracy,\n",
        "            'initial_features': len(selected_features['TB_Stage']),\n",
        "            'final_features': X_test_selected.shape[1],\n",
        "            'samples': len(y_test),\n",
        "            'dataset': 'ATB_LTB'\n",
        "        }\n",
        "\n",
        "        detailed_results['TB_Stage'] = {\n",
        "            'true_labels': y_test,\n",
        "            'pred_labels': y_pred,\n",
        "            'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
        "        }\n",
        "\n",
        "        print(f\"   ‚úÖ Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"   ‚úÖ Features: {len(selected_features['TB_Stage'])} ‚Üí {X_test_selected.shape[1]}\")\n",
        "        print(f\"   ‚úÖ Test Samples: {len(y_test)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error in TB Stage testing: {e}\")\n",
        "        results['TB_Stage'] = {'error': str(e)}\n",
        "\n",
        "    return results, detailed_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8zPT9nK_arer"
      },
      "outputs": [],
      "source": [
        "def create_internal_testing_visualizations(results, detailed_results):\n",
        "    \"\"\"Create comprehensive visualizations for internal testing\"\"\"\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = r'..\\..\\Saved_files\\internal_testing'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Filter out models with errors\n",
        "    valid_results = {k: v for k, v in results.items() if 'error' not in v}\n",
        "\n",
        "    if not valid_results:\n",
        "        print(\"‚ùå No valid results to visualize\")\n",
        "        return\n",
        "\n",
        "    # 1. MAIN PERFORMANCE DASHBOARD\n",
        "    fig1, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
        "    fig1.suptitle('INTERNAL TESTING - TB DIAGNOSTIC SYSTEM PERFORMANCE',\n",
        "                 fontsize=20, fontweight='bold', y=0.98)\n",
        "\n",
        "    model_names = list(valid_results.keys())\n",
        "    accuracies = [valid_results[model]['accuracy'] for model in model_names]\n",
        "    initial_features = [valid_results[model]['initial_features'] for model in model_names]\n",
        "    final_features = [valid_results[model]['final_features'] for model in model_names]\n",
        "    samples = [valid_results[model]['samples'] for model in model_names]\n",
        "\n",
        "    # 1.1 Accuracy Comparison (Top-left)\n",
        "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
        "    bars = ax1.bar(model_names, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "    ax1.set_title('Model Performance by Diagnostic Level', fontsize=16, fontweight='bold', pad=20)\n",
        "    ax1.set_ylabel('Accuracy', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylim(0, 1.0)\n",
        "    ax1.tick_params(axis='x', labelsize=12)\n",
        "    ax1.tick_params(axis='y', labelsize=12)\n",
        "    ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "    # Add value labels\n",
        "    for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold',\n",
        "                fontsize=13, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
        "\n",
        "    # 1.2 Feature Efficiency (Top-right)\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.35\n",
        "\n",
        "    bars_initial = ax2.bar(x - width/2, initial_features, width,\n",
        "                          label='Before Selection', color='lightblue',\n",
        "                          alpha=0.7, edgecolor='navy')\n",
        "    bars_final = ax2.bar(x + width/2, final_features, width,\n",
        "                        label='After Selection', color='lightcoral',\n",
        "                        alpha=0.7, edgecolor='darkred')\n",
        "\n",
        "    ax2.set_title('Feature Selection Efficiency', fontsize=16, fontweight='bold', pad=20)\n",
        "    ax2.set_ylabel('Number of Biomarkers', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels(model_names, fontsize=12)\n",
        "    ax2.legend(fontsize=12)\n",
        "    ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "    # Add reduction percentages\n",
        "    for i, (initial, final) in enumerate(zip(initial_features, final_features)):\n",
        "        reduction_pct = (initial - final) / initial * 100\n",
        "        ax2.text(i, final + 1, f'-{reduction_pct:.0f}%',\n",
        "                ha='center', va='bottom', fontweight='bold', fontsize=11,\n",
        "                bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='yellow', alpha=0.7))\n",
        "\n",
        "    # 1.3 Sample Distribution (Bottom-left)\n",
        "    wedges, texts, autotexts = ax3.pie(samples, labels=model_names, autopct='%1.1f%%',\n",
        "                                      colors=colors, startangle=90,\n",
        "                                      textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
        "    ax3.set_title('Sample Distribution Across Diagnostic Levels', fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "    # 1.4 Performance Summary (Bottom-right)\n",
        "    ax4.axis('off')\n",
        "    summary_text = \"INTERNAL TESTING SUMMARY\\n\\n\"\n",
        "    for model_name in model_names:\n",
        "        data = valid_results[model_name]\n",
        "        reduction_pct = (data['initial_features'] - data['final_features']) / data['initial_features'] * 100\n",
        "        summary_text += f\"‚Ä¢ {model_name}: {data['accuracy']:.3f} accuracy\\n\"\n",
        "        summary_text += f\"  Features: {data['initial_features']} ‚Üí {data['final_features']} (-{reduction_pct:.1f}%)\\n\"\n",
        "        summary_text += f\"  Samples: {data['samples']}\\n\\n\"\n",
        "\n",
        "    ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, fontsize=12,\n",
        "            verticalalignment='top', bbox=dict(boxstyle=\"round,pad=1\", facecolor='lightblue', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{output_dir}/01_Performance_Dashboard.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. CONFUSION MATRICES (for TB Status only)\n",
        "    if 'TB_Status' in detailed_results:\n",
        "        fig2, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "        y_true = detailed_results['TB_Status']['true_labels']\n",
        "        y_pred = detailed_results['TB_Status']['pred_labels']\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        classes = sorted(set(y_true) | set(y_pred))\n",
        "\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                   xticklabels=classes, yticklabels=classes,\n",
        "                   cbar_kws={'shrink': 0.8})\n",
        "\n",
        "        ax.set_title(f'TB Status - Confusion Matrix\\nAccuracy: {valid_results[\"TB_Status\"][\"accuracy\"]:.3f}',\n",
        "                    fontsize=16, fontweight='bold')\n",
        "        ax.set_xlabel('Predicted Labels', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('True Labels', fontsize=12, fontweight='bold')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{output_dir}/02_Confusion_Matrix_TB_Status.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    # 3. FEATURE REDUCTION IMPACT\n",
        "    fig3, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    reduction_percentages = []\n",
        "    for model_name in model_names:\n",
        "        data = valid_results[model_name]\n",
        "        reduction_pct = (data['initial_features'] - data['final_features']) / data['initial_features'] * 100\n",
        "        reduction_percentages.append(reduction_pct)\n",
        "\n",
        "    bars = ax.bar(model_names, reduction_percentages, color=colors, alpha=0.8, edgecolor='black')\n",
        "    ax.set_title('Feature Reduction Impact', fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.set_ylabel('Feature Reduction (%)', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Diagnostic Model', fontsize=14, fontweight='bold')\n",
        "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, pct in zip(bars, reduction_percentages):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                f'{pct:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{output_dir}/03_Feature_Reduction_Impact.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 4. PERFORMANCE SUMMARY TABLE\n",
        "    fig4, ax = plt.subplots(figsize=(12, 4))\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "\n",
        "    table_data = []\n",
        "    headers = ['Model', 'Accuracy', 'Initial Features', 'Final Features', 'Reduction (%)', 'Samples']\n",
        "\n",
        "    for model_name in model_names:\n",
        "        data = valid_results[model_name]\n",
        "        reduction_pct = (data['initial_features'] - data['final_features']) / data['initial_features'] * 100\n",
        "\n",
        "        table_data.append([\n",
        "            model_name,\n",
        "            f\"{data['accuracy']:.3f}\",\n",
        "            data['initial_features'],\n",
        "            data['final_features'],\n",
        "            f\"{reduction_pct:.1f}%\",\n",
        "            data['samples']\n",
        "        ])\n",
        "\n",
        "    table = ax.table(cellText=table_data, colLabels=headers,\n",
        "                    cellLoc='center', loc='center',\n",
        "                    bbox=[0, 0, 1, 1])\n",
        "\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(11)\n",
        "    table.scale(1, 2)\n",
        "\n",
        "    # Color headers\n",
        "    for i in range(len(headers)):\n",
        "        table[(0, i)].set_facecolor('#2E86AB')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "    ax.set_title('INTERNAL TESTING SUMMARY TABLE', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{output_dir}/04_Testing_Summary_Table.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"‚úÖ All internal testing visualizations saved to: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CIR-oJwIares"
      },
      "outputs": [],
      "source": [
        "def save_internal_testing_report(results):\n",
        "    \"\"\"Save detailed internal testing report\"\"\"\n",
        "    report_dir = r'..\\..\\Saved_files\\internal_testing'\n",
        "\n",
        "    # Create detailed results DataFrame\n",
        "    report_data = []\n",
        "    for model_name, result in results.items():\n",
        "        if 'error' not in result:\n",
        "            report_data.append({\n",
        "                'Model': model_name,\n",
        "                'Dataset': result.get('dataset', 'Unknown'),\n",
        "                'Accuracy': result['accuracy'],\n",
        "                'Initial_Features': result['initial_features'],\n",
        "                'Final_Features': result['final_features'],\n",
        "                'Feature_Reduction_Percent': (result['initial_features'] - result['final_features']) / result['initial_features'] * 100,\n",
        "                'Samples': result['samples'],\n",
        "                'Note': result.get('note', '')\n",
        "            })\n",
        "\n",
        "    report_df = pd.DataFrame(report_data)\n",
        "    report_df.to_csv(f'{report_dir}/internal_testing_report.csv', index=False)\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    valid_results = [r for r in results.values() if 'error' not in r]\n",
        "    if valid_results:\n",
        "        summary = {\n",
        "            'Total_Models_Tested': len(valid_results),\n",
        "            'Mean_Accuracy': np.mean([r['accuracy'] for r in valid_results]),\n",
        "            'Total_Initial_Features': sum([r['initial_features'] for r in valid_results]),\n",
        "            'Total_Final_Features': sum([r['final_features'] for r in valid_results]),\n",
        "            'Overall_Feature_Reduction_Percent': (\n",
        "                sum([r['initial_features'] for r in valid_results]) -\n",
        "                sum([r['final_features'] for r in valid_results])\n",
        "            ) / sum([r['initial_features'] for r in valid_results]) * 100,\n",
        "            'Total_Samples': sum([r['samples'] for r in valid_results])\n",
        "        }\n",
        "\n",
        "        summary_df = pd.DataFrame([summary])\n",
        "        summary_df.to_csv(f'{report_dir}/internal_testing_summary.csv', index=False)\n",
        "\n",
        "    print(f\"‚úÖ Internal testing reports saved to {report_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vfB8yQdCaret",
        "outputId": "c2cccad8-c07b-4f99-a5cd-de4f874bb1fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ STARTING CORRECTED INTERNAL TESTING\n",
            "============================================================\n",
            "üî¨ STARTING CORRECTED INTERNAL TESTING ANALYSIS\n",
            "============================================================\n",
            "‚ùå Error loading components: [Errno 2] No such file or directory: '/content/drive/MyDrive/Research/TB_new/Saved_files/best_sub_model_2.pkl'\n",
            "‚ùå Internal testing failed\n"
          ]
        }
      ],
      "source": [
        "# Execute Corrected Internal Testing\n",
        "print(\"üöÄ STARTING CORRECTED INTERNAL TESTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results, detailed_results = perform_corrected_internal_testing()\n",
        "\n",
        "if results:\n",
        "    print(\"\\nüìà CREATING VISUALIZATIONS...\")\n",
        "    create_internal_testing_visualizations(results, detailed_results)\n",
        "\n",
        "    print(\"\\nüíæ SAVING REPORTS...\")\n",
        "    save_internal_testing_report(results)\n",
        "\n",
        "    # Print summary\n",
        "    valid_results = {k: v for k, v in results.items() if 'error' not in v}\n",
        "    if valid_results:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üéâ CORRECTED INTERNAL TESTING COMPLETE!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        accuracies = [v['accuracy'] for v in valid_results.values()]\n",
        "        initial_features = [v['initial_features'] for v in valid_results.values()]\n",
        "        final_features = [v['final_features'] for v in valid_results.values()]\n",
        "\n",
        "        print(f\"\"\"\n",
        "üìä INTERNAL TESTING HIGHLIGHTS:\n",
        "\n",
        "üèÜ PERFORMANCE METRICS:\n",
        "   ‚Ä¢ Mean Accuracy: {np.mean(accuracies):.3f}\n",
        "   ‚Ä¢ Total Features: {sum(initial_features)} ‚Üí {sum(final_features)}\n",
        "   ‚Ä¢ Overall Feature Reduction: {(sum(initial_features) - sum(final_features)) / sum(initial_features) * 100:.1f}%\n",
        "   ‚Ä¢ Total Test Samples: {sum([v['samples'] for v in valid_results.values()])}\n",
        "\n",
        "üí° MODEL-SPECIFIC RESULTS:\"\"\")\n",
        "\n",
        "        for model_name, result in valid_results.items():\n",
        "            reduction_pct = (result['initial_features'] - result['final_features']) / result['initial_features'] * 100\n",
        "            print(f\"   ‚Ä¢ {model_name}: {result['accuracy']:.3f} accuracy\")\n",
        "            print(f\"     Features: {result['initial_features']} ‚Üí {result['final_features']} (-{reduction_pct:.1f}%)\")\n",
        "            print(f\"     Test Samples: {result['samples']}\")\n",
        "\n",
        "        print(f\"\"\"\n",
        "üìÅ OUTPUTS GENERATED:\n",
        "   ‚Ä¢ Professional visualizations in 'Saved_files/internal_testing/'\n",
        "   ‚Ä¢ Detailed report in 'Saved_files/internal_testing/internal_testing_report.csv'\n",
        "   ‚Ä¢ Summary statistics in 'Saved_files/internal_testing/internal_testing_summary.csv'\n",
        "\n",
        "üéØ READY FOR EXTERNAL VALIDATION!\n",
        "\"\"\")\n",
        "    else:\n",
        "        print(\"‚ùå No valid results to summarize\")\n",
        "else:\n",
        "    print(\"‚ùå Internal testing failed\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}