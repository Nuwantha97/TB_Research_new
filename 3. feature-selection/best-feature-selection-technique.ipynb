{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Feature Selection Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 3 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "TB_HC_OD = pd.read_csv(r'..\\..\\Datasets\\train\\balanced\\TB_HC_OD.csv')\n",
    "PTB_EPTB = pd.read_csv(r'..\\..\\Datasets\\train\\balanced\\PTB_EPTB.csv')\n",
    "ATB_LTB = pd.read_csv(r'..\\..\\Datasets\\train\\balanced\\ATB_LTB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate features and target\n",
    "X_TB_HC_OD = TB_HC_OD.drop(columns=['TB_Status'])\n",
    "y_TB_HC_OD = TB_HC_OD['TB_Status']\n",
    "\n",
    "X_PTB_EPTB = PTB_EPTB.drop(columns=['TB_Status'])\n",
    "y_PTB_EPTB = PTB_EPTB['TB_Status']\n",
    "\n",
    "X_ATB_LTB = ATB_LTB.drop(columns=['TB_Status'])\n",
    "y_ATB_LTB = ATB_LTB['TB_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels\n",
    "def encode_target(y):\n",
    "    le = LabelEncoder()\n",
    "    return le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Best Feature Selection Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED function to evaluate feature selection methods\n",
    "def evaluate_feature_selection_optimized(X, y, dataset_name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Evaluating {dataset_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    y_encoded = encode_target(y)\n",
    "    \n",
    "    # Reduced feature counts for faster execution\n",
    "    if dataset_name == \"TB_HC_OD\":\n",
    "        feature_counts = [15, 20, 25, 30, 35, 40]\n",
    "    elif dataset_name == \"PTB_EPTB\":\n",
    "        feature_counts = [8, 10, 12, 15, 18, 20]\n",
    "    else:  # ATB_LTB\n",
    "        feature_counts = [5, 8, 10, 12, 15]\n",
    "    \n",
    "    results = []\n",
    "    best_score = 0\n",
    "    best_config = {}\n",
    "    \n",
    "    # Use only fastest algorithms\n",
    "    algorithms = {\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1),\n",
    "        'GradientBoosting': GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
    "    }\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for algo_name, model in algorithms.items():\n",
    "        print(f\"\\nTesting {algo_name}...\")\n",
    "        \n",
    "        for k in feature_counts:\n",
    "            try:\n",
    "                # Fast feature importance calculation\n",
    "                model.fit(X, y_encoded)\n",
    "                feature_importances = model.feature_importances_\n",
    "                top_k_indices = np.argsort(feature_importances)[-k:]\n",
    "                X_selected = X.iloc[:, top_k_indices]\n",
    "                \n",
    "                # Quick cross-validation with reduced folds\n",
    "                score = cross_val_score(\n",
    "                    RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1),\n",
    "                    X_selected, y_encoded, cv=kf, n_jobs=-1\n",
    "                ).mean()\n",
    "                \n",
    "                results.append({\n",
    "                    'Algorithm': algo_name,\n",
    "                    'Feature_Count': k,\n",
    "                    'CV_Score': score\n",
    "                })\n",
    "                \n",
    "                print(f\"  Features: {k}, CV Score: {score:.4f}\")\n",
    "                \n",
    "                # Track best configuration\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_config = {'algorithm': algo_name, 'features': k, 'score': score}\n",
    "                    \n",
    "                # Early stopping if we reach 90%+\n",
    "                if score >= 0.90:\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error with {k} features: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Also test SelectKBest for comparison\n",
    "    print(f\"\\nTesting SelectKBest...\")\n",
    "    for k in feature_counts[:3]:  # Test only first 3 for speed\n",
    "        try:\n",
    "            selector = SelectKBest(score_func=f_classif, k=k)\n",
    "            X_selected = selector.fit_transform(X, y_encoded)\n",
    "            \n",
    "            score = cross_val_score(\n",
    "                RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1),\n",
    "                X_selected, y_encoded, cv=kf, n_jobs=-1\n",
    "            ).mean()\n",
    "            \n",
    "            results.append({\n",
    "                'Algorithm': 'SelectKBest_f_classif',\n",
    "                'Feature_Count': k,\n",
    "                'CV_Score': score\n",
    "            })\n",
    "            \n",
    "            print(f\"  Features: {k}, CV Score: {score:.4f}\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_config = {'algorithm': 'SelectKBest_f_classif', 'features': k, 'score': score}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with {k} features: {e}\")\n",
    "            continue\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nâ° Execution time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Display best result\n",
    "    print(f\"\\nðŸŽ¯ BEST CONFIGURATION for {dataset_name}:\")\n",
    "    print(f\"   Algorithm: {best_config['algorithm']}\")\n",
    "    print(f\"   Feature Count: {best_config['features']}\")\n",
    "    print(f\"   CV Score: {best_config['score']:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(results), best_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ STARTING OPTIMIZED FEATURE SELECTION EVALUATION\n",
      "Target: â‰¥90% CV Score with Minimal Features\n",
      "Total Feature Budget: â‰¤50 across all datasets\n",
      "\n",
      "==================================================\n",
      "Evaluating TB_HC_OD\n",
      "==================================================\n",
      "\n",
      "Testing RandomForest...\n",
      "  Features: 15, CV Score: 0.7473\n",
      "  Features: 20, CV Score: 0.7524\n",
      "  Features: 25, CV Score: 0.7793\n",
      "  Features: 30, CV Score: 0.7918\n",
      "  Features: 35, CV Score: 0.7941\n",
      "  Features: 40, CV Score: 0.7955\n",
      "\n",
      "Testing GradientBoosting...\n",
      "  Features: 15, CV Score: 0.8006\n",
      "  Features: 20, CV Score: 0.8173\n",
      "  Features: 25, CV Score: 0.8141\n",
      "  Features: 30, CV Score: 0.8229\n",
      "  Features: 35, CV Score: 0.8322\n",
      "  Features: 40, CV Score: 0.8321\n",
      "\n",
      "Testing SelectKBest...\n",
      "  Features: 15, CV Score: 0.7353\n",
      "  Features: 20, CV Score: 0.7460\n",
      "  Features: 25, CV Score: 0.7640\n",
      "\n",
      "â° Execution time: 16969.99 seconds\n",
      "\n",
      "ðŸŽ¯ BEST CONFIGURATION for TB_HC_OD:\n",
      "   Algorithm: GradientBoosting\n",
      "   Feature Count: 35\n",
      "   CV Score: 0.8322\n",
      "\n",
      "==================================================\n",
      "Evaluating PTB_EPTB\n",
      "==================================================\n",
      "\n",
      "Testing RandomForest...\n",
      "  Features: 8, CV Score: 0.6657\n",
      "  Features: 10, CV Score: 0.6658\n",
      "  Features: 12, CV Score: 0.6752\n",
      "  Features: 15, CV Score: 0.6683\n",
      "  Features: 18, CV Score: 0.6870\n",
      "  Features: 20, CV Score: 0.7036\n",
      "\n",
      "Testing GradientBoosting...\n",
      "  Features: 8, CV Score: 0.6964\n",
      "  Features: 10, CV Score: 0.7201\n",
      "  Features: 12, CV Score: 0.7463\n",
      "  Features: 15, CV Score: 0.7604\n",
      "  Features: 18, CV Score: 0.7629\n",
      "  Features: 20, CV Score: 0.7653\n",
      "\n",
      "Testing SelectKBest...\n",
      "  Features: 8, CV Score: 0.6823\n",
      "  Features: 10, CV Score: 0.6894\n",
      "  Features: 12, CV Score: 0.6729\n",
      "\n",
      "â° Execution time: 1130.14 seconds\n",
      "\n",
      "ðŸŽ¯ BEST CONFIGURATION for PTB_EPTB:\n",
      "   Algorithm: GradientBoosting\n",
      "   Feature Count: 20\n",
      "   CV Score: 0.7653\n",
      "\n",
      "==================================================\n",
      "Evaluating ATB_LTB\n",
      "==================================================\n",
      "\n",
      "Testing RandomForest...\n",
      "  Features: 5, CV Score: 0.8250\n",
      "  Features: 8, CV Score: 0.8583\n",
      "  Features: 10, CV Score: 0.8528\n",
      "  Features: 12, CV Score: 0.8639\n",
      "  Features: 15, CV Score: 0.8750\n",
      "\n",
      "Testing GradientBoosting...\n",
      "  Features: 5, CV Score: 0.8472\n",
      "  Features: 8, CV Score: 0.8889\n",
      "  Features: 10, CV Score: 0.9028\n",
      "\n",
      "Testing SelectKBest...\n",
      "  Features: 5, CV Score: 0.8028\n",
      "  Features: 8, CV Score: 0.8167\n",
      "  Features: 10, CV Score: 0.8250\n",
      "\n",
      "â° Execution time: 590.88 seconds\n",
      "\n",
      "ðŸŽ¯ BEST CONFIGURATION for ATB_LTB:\n",
      "   Algorithm: GradientBoosting\n",
      "   Feature Count: 10\n",
      "   CV Score: 0.9028\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all datasets\n",
    "print(\"ðŸš€ STARTING OPTIMIZED FEATURE SELECTION EVALUATION\")\n",
    "print(\"Target: â‰¥90% CV Score with Minimal Features\")\n",
    "print(\"Total Feature Budget: â‰¤50 across all datasets\")\n",
    "\n",
    "results_TB_HC_OD, best_TB_HC_OD = evaluate_feature_selection_optimized(X_TB_HC_OD, y_TB_HC_OD, \"TB_HC_OD\")\n",
    "results_PTB_EPTB, best_PTB_EPTB = evaluate_feature_selection_optimized(X_PTB_EPTB, y_PTB_EPTB, \"PTB_EPTB\")\n",
    "results_ATB_LTB, best_ATB_LTB = evaluate_feature_selection_optimized(X_ATB_LTB, y_ATB_LTB, \"ATB_LTB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š FINAL SUMMARY:\n",
      "TB_HC_OD: 35 features, CV: 0.8322\n",
      "PTB_EPTB: 20 features, CV: 0.7653\n",
      "ATB_LTB: 10 features, CV: 0.9028\n",
      "TOTAL FEATURES: 65 (Target: â‰¤50)\n",
      "ALL â‰¥90%: False\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "total_features = best_TB_HC_OD['features'] + best_PTB_EPTB['features'] + best_ATB_LTB['features']\n",
    "print(f\"\\nðŸ“Š FINAL SUMMARY:\")\n",
    "print(f\"TB_HC_OD: {best_TB_HC_OD['features']} features, CV: {best_TB_HC_OD['score']:.4f}\")\n",
    "print(f\"PTB_EPTB: {best_PTB_EPTB['features']} features, CV: {best_PTB_EPTB['score']:.4f}\")\n",
    "print(f\"ATB_LTB: {best_ATB_LTB['features']} features, CV: {best_ATB_LTB['score']:.4f}\")\n",
    "print(f\"TOTAL FEATURES: {total_features} (Target: â‰¤50)\")\n",
    "print(f\"ALL â‰¥90%: {all([best_TB_HC_OD['score'] >= 0.90, best_PTB_EPTB['score'] >= 0.90, best_ATB_LTB['score'] >= 0.90])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Best configurations saved!\n"
     ]
    }
   ],
   "source": [
    "# Save best configurations for feature selection\n",
    "best_configs = {\n",
    "    'TB_HC_OD': best_TB_HC_OD,\n",
    "    'PTB_EPTB': best_PTB_EPTB, \n",
    "    'ATB_LTB': best_ATB_LTB\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(r'..\\..\\Saved_files\\best_feature_configs.json', 'w') as f:\n",
    "    json.dump(best_configs, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Best configurations saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMIZING PTB_EPTB FOR â‰¥80% CV SCORE\n",
      "============================================================\n",
      "\n",
      "ðŸ” Testing GradientBoosting...\n",
      "   Features:  25, CV Score: 0.7748 \n",
      "   Features:  30, CV Score: 0.7653 \n",
      "   Features:  35, CV Score: 0.7724 \n",
      "   Features:  40, CV Score: 0.7629 \n",
      "   Features:  45, CV Score: 0.7938 \n",
      "   Features:  50, CV Score: 0.7913 \n",
      "   Features:  60, CV Score: 0.7771 \n",
      "\n",
      "ðŸ” Testing RandomForest...\n",
      "   Features:  25, CV Score: 0.6847 \n",
      "   Features:  30, CV Score: 0.6659 \n",
      "   Features:  35, CV Score: 0.6730 \n",
      "   Features:  40, CV Score: 0.6944 \n",
      "   Features:  45, CV Score: 0.6872 \n",
      "   Features:  50, CV Score: 0.6778 \n",
      "   Features:  60, CV Score: 0.7038 \n",
      "\n",
      "ðŸ” Testing SelectKBest_f_classif...\n",
      "   Error with 25 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'f_classif' instead.\n",
      "   Error with 30 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'f_classif' instead.\n",
      "   Error with 35 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'f_classif' instead.\n",
      "   Error with 40 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'f_classif' instead.\n",
      "   Error with 45 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'f_classif' instead.\n",
      "   Error with 50 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'f_classif' instead.\n",
      "   Error with 60 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'f_classif' instead.\n",
      "\n",
      "ðŸ” Testing SelectKBest_mutual_info...\n",
      "   Error with 25 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'mutual_info_classif' instead.\n",
      "   Error with 30 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'mutual_info_classif' instead.\n",
      "   Error with 35 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'mutual_info_classif' instead.\n",
      "   Error with 40 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'mutual_info_classif' instead.\n",
      "   Error with 45 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'mutual_info_classif' instead.\n",
      "   Error with 50 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'mutual_info_classif' instead.\n",
      "   Error with 60 features: The 'score_func' parameter of SelectKBest must be a callable. Got 'mutual_info_classif' instead.\n",
      "\n",
      "â° Optimization time: 2836.12 seconds\n",
      "\n",
      "ðŸ“Š RESULTS SUMMARY for PTB_EPTB:\n",
      "   Best Algorithm: GradientBoosting\n",
      "   Best Feature Count: 45\n",
      "   Best CV Score: 0.7938\n",
      "\n",
      "ðŸ† TOP 5 CONFIGURATIONS:\n",
      "   GradientBoosting          |  45 features | CV: 0.7938\n",
      "   GradientBoosting          |  50 features | CV: 0.7913\n",
      "   GradientBoosting          |  60 features | CV: 0.7771\n",
      "   GradientBoosting          |  25 features | CV: 0.7748\n",
      "   GradientBoosting          |  35 features | CV: 0.7724\n",
      "\n",
      "âœ… Updated configurations saved!\n",
      "ðŸ“ˆ Previous PTB_EPTB score: 0.7653\n",
      "ðŸ“ˆ Optimized PTB_EPTB score: 0.7938\n",
      "ðŸ“Š TOTAL FEATURES: 90\n",
      "ðŸŽ¯ TARGET ACHIEVED: False\n"
     ]
    }
   ],
   "source": [
    "## Optimize PTB_EPTB Dataset Only - Extended Feature Range\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "\n",
    "# Load PTB_EPTB dataset only\n",
    "PTB_EPTB = pd.read_csv(r'..\\..\\Datasets\\train\\balanced\\PTB_EPTB.csv')\n",
    "X_PTB_EPTB = PTB_EPTB.drop(columns=['TB_Status'])\n",
    "y_PTB_EPTB = PTB_EPTB['TB_Status']\n",
    "\n",
    "# Encode target labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_PTB_EPTB)\n",
    "\n",
    "def optimize_ptb_eptb_features(X, y_encoded, dataset_name=\"PTB_EPTB\"):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"OPTIMIZING {dataset_name} FOR â‰¥80% CV SCORE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extended feature range testing\n",
    "    feature_counts = [25, 30, 35, 40, 45, 50, 60]\n",
    "    \n",
    "    results = []\n",
    "    best_score = 0\n",
    "    best_config = {}\n",
    "    \n",
    "    algorithms = {\n",
    "        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        'SelectKBest_f_classif': 'f_classif',\n",
    "        'SelectKBest_mutual_info': 'mutual_info_classif'\n",
    "    }\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for algo_name, algorithm in algorithms.items():\n",
    "        print(f\"\\nðŸ” Testing {algo_name}...\")\n",
    "        \n",
    "        for k in feature_counts:\n",
    "            try:\n",
    "                if algo_name in ['GradientBoosting', 'RandomForest']:\n",
    "                    # Tree-based feature importance\n",
    "                    model = algorithm\n",
    "                    model.fit(X, y_encoded)\n",
    "                    feature_importances = model.feature_importances_\n",
    "                    top_k_indices = np.argsort(feature_importances)[-k:]\n",
    "                    X_selected = X.iloc[:, top_k_indices]\n",
    "                    \n",
    "                else:\n",
    "                    # SelectKBest with different scoring functions\n",
    "                    selector = SelectKBest(score_func=algorithm, k=k)\n",
    "                    X_selected = selector.fit_transform(X, y_encoded)\n",
    "                \n",
    "                # Cross-validation with RandomForest\n",
    "                score = cross_val_score(\n",
    "                    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "                    X_selected, y_encoded, cv=kf, n_jobs=-1\n",
    "                ).mean()\n",
    "                \n",
    "                results.append({\n",
    "                    'Algorithm': algo_name,\n",
    "                    'Feature_Count': k,\n",
    "                    'CV_Score': score\n",
    "                })\n",
    "                \n",
    "                print(f\"   Features: {k:3d}, CV Score: {score:.4f} {'âœ…' if score >= 0.80 else ''}\")\n",
    "                \n",
    "                # Track best configuration\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_config = {\n",
    "                        'algorithm': algo_name, \n",
    "                        'features': k, \n",
    "                        'score': score\n",
    "                    }\n",
    "                \n",
    "                # Early stopping if we reach target\n",
    "                if score >= 0.80:\n",
    "                    print(f\"   ðŸŽ¯ TARGET ACHIEVED with {k} features!\")\n",
    "                    # Continue to see if we can do better with fewer features\n",
    "                    if k <= 30:  # If we hit target with â‰¤30 features, we're good\n",
    "                        break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   Error with {k} features: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Stop testing this algorithm if we found a good solution\n",
    "        if best_score >= 0.80 and best_config['features'] <= 40:\n",
    "            print(f\"   âœ… Optimal solution found with {algo_name}\")\n",
    "            break\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nâ° Optimization time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Display detailed results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š RESULTS SUMMARY for {dataset_name}:\")\n",
    "    print(f\"   Best Algorithm: {best_config['algorithm']}\")\n",
    "    print(f\"   Best Feature Count: {best_config['features']}\")\n",
    "    print(f\"   Best CV Score: {best_config['score']:.4f}\")\n",
    "    \n",
    "    # Show top 5 configurations\n",
    "    top_5 = results_df.nlargest(5, 'CV_Score')\n",
    "    print(f\"\\nðŸ† TOP 5 CONFIGURATIONS:\")\n",
    "    for idx, row in top_5.iterrows():\n",
    "        print(f\"   {row['Algorithm']:25} | {row['Feature_Count']:3d} features | CV: {row['CV_Score']:.4f}\")\n",
    "    \n",
    "    return results_df, best_config\n",
    "\n",
    "# Run optimization for PTB_EPTB\n",
    "results_PTB_EPTB_optimized, best_PTB_EPTB_optimized = optimize_ptb_eptb_features(X_PTB_EPTB, y_encoded)\n",
    "\n",
    "# Update the best configurations\n",
    "best_configs_updated = {\n",
    "    'TB_HC_OD': {'algorithm': 'GradientBoosting', 'features': 35, 'score': 0.8322},\n",
    "    'PTB_EPTB': best_PTB_EPTB_optimized,\n",
    "    'ATB_LTB': {'algorithm': 'GradientBoosting', 'features': 10, 'score': 0.9028}\n",
    "}\n",
    "\n",
    "# Save updated configurations\n",
    "import json\n",
    "with open(r'..\\..\\Saved_files\\best_feature_configs_updated.json', 'w') as f:\n",
    "    json.dump(best_configs_updated, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Updated configurations saved!\")\n",
    "print(f\"ðŸ“ˆ Previous PTB_EPTB score: 0.7653\")\n",
    "print(f\"ðŸ“ˆ Optimized PTB_EPTB score: {best_PTB_EPTB_optimized['score']:.4f}\")\n",
    "\n",
    "# Calculate total features\n",
    "total_features = (best_configs_updated['TB_HC_OD']['features'] + \n",
    "                 best_configs_updated['PTB_EPTB']['features'] + \n",
    "                 best_configs_updated['ATB_LTB']['features'])\n",
    "\n",
    "print(f\"ðŸ“Š TOTAL FEATURES: {total_features}\")\n",
    "print(f\"ðŸŽ¯ TARGET ACHIEVED: {best_PTB_EPTB_optimized['score'] >= 0.80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
